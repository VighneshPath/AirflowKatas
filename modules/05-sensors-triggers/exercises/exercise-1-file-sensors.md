# Exercise 1: File Sensors

## Objective

Learn to implement and configure FileSensors with different timeout and retry settings.

## Background

You're building a data pipeline that processes daily sales reports. The reports are generated by an external system and saved to specific directories. Your pipeline needs to wait for these files before processing can begin.

## Tasks

### Task 1: Basic FileSensor Implementation

Create a DAG named `daily_sales_pipeline` that:

1. Waits for a file named `sales_report_YYYYMMDD.csv` in `/tmp/sales-data/`
2. Uses a 30-second poke interval
3. Has a 5-minute timeout
4. Processes the file once detected (simple bash command to display file contents)

**Requirements:**

- DAG should run daily at 9:00 AM
- Use appropriate tags: `['exercise', 'file-sensor', 'sales']`
- Include proper error handling

### Task 2: Multiple File Dependencies

Extend your pipeline to wait for multiple files:

1. `sales_report_YYYYMMDD.csv` - main sales data
2. `customer_data_YYYYMMDD.json` - customer information
3. `product_catalog_YYYYMMDD.xml` - product details

**Requirements:**

- All three files must be present before processing begins
- Use different timeout values based on file importance:
  - Sales report: 10 minutes (critical)
  - Customer data: 15 minutes (important)
  - Product catalog: 20 minutes (can be delayed)
- Implement parallel sensor execution where possible

### Task 3: Advanced Sensor Configuration

Create a robust sensor configuration with:

1. **Reschedule mode** to avoid blocking worker slots
2. **Exponential backoff** for retries
3. **Custom callbacks** for success and failure scenarios
4. **SLA monitoring** with 30-minute SLA

**Requirements:**

- Use reschedule mode for sensors with long wait times
- Implement 3 retries with exponential backoff
- Create callback functions that log detailed information
- Set up SLA monitoring and callbacks

## Starter Code

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'your-name',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Your DAG implementation here
```

## Test Data Setup

Create test files using this helper function:

```python
def create_test_files():
    """Create test files for sensor exercises"""
    import os
    from datetime import datetime

    # Create directory
    os.makedirs('/tmp/sales-data', exist_ok=True)

    # Create files with current date
    today = datetime.now().strftime('%Y%m%d')

    files = {
        f'/tmp/sales-data/sales_report_{today}.csv': 'date,product,amount\n2024-01-01,Widget,100\n2024-01-01,Gadget,200',
        f'/tmp/sales-data/customer_data_{today}.json': '{"customers": [{"id": 1, "name": "John Doe"}]}',
        f'/tmp/sales-data/product_catalog_{today}.xml': '<products><product id="1">Widget</product></products>'
    }

    for file_path, content in files.items():
        with open(file_path, 'w') as f:
            f.write(content)
        print(f"Created: {file_path}")
```

## Expected Behavior

1. **Task 1**: DAG should wait for the sales report file and process it once available
2. **Task 2**: All three sensors should run in parallel, and processing should only start when all files are detected
3. **Task 3**: Sensors should use reschedule mode, implement proper retry logic, and trigger callbacks appropriately

## Validation Criteria

- [ ] DAG parses without errors
- [ ] Sensors are configured with correct file paths and timeouts
- [ ] Task dependencies are properly set
- [ ] Callbacks are implemented and functional
- [ ] SLA monitoring is configured
- [ ] Test files can be detected by sensors

## Troubleshooting Tips

1. **File not found**: Ensure test files are created with correct naming pattern
2. **Permission errors**: Check file system permissions for `/tmp/sales-data/`
3. **Timeout issues**: Adjust timeout values if sensors fail too quickly
4. **Worker slot exhaustion**: Use reschedule mode for long-running sensors

## Extension Challenges

1. **Dynamic file patterns**: Modify sensors to handle files from previous days if current day's files are missing
2. **File validation**: Add checks to ensure files are not empty and have valid formats
3. **Notification system**: Implement email or Slack notifications for sensor failures
4. **Monitoring dashboard**: Create a simple monitoring system to track sensor performance

## Solution Location

Check your implementation against the solution in `solutions/exercise-1-solution.py`
